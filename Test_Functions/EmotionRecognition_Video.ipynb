{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s571HXMBOIcu"
   },
   "source": [
    "# Ziad Al-Ziadi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGWXTDHjOLe9"
   },
   "source": [
    "## Testing our CNN model with the EmotionRecognition function\n",
    "\n",
    "In the preceding section, we define the EmotionRecognition function to use with the CNN we built and trained previously. The CNN will be loaded in as a pickle file.\n",
    "\n",
    "#NOTE: This notebook was built with GPU and CUDA enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbLDI6ILPs4v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDQAXfUVTPHa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxlF2zJoAMBC"
   },
   "outputs": [],
   "source": [
    "####### PLEASE DRAG THE \"CV_Submission\" FROM YOUR \"Shared with me\" INTO YOUR \"My Drive\" OTHERWISE THIS WASN'T WORK #######\n",
    "\n",
    "import os\n",
    "\n",
    "MY_OWN_PATH = \"/content/drive/MyDrive/CV_Submission/\"\n",
    "MY_OWN_PATH = \"CV_Submission\"\n",
    "FULL_PATH = os.path.join(\"drive\", \"My Drive\", MY_OWN_PATH)\n",
    "print(os.listdir(FULL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Luw_Uq0iPWtU"
   },
   "outputs": [],
   "source": [
    "# Defining our function which takes a model and test dataframe\n",
    "\n",
    "def EmotionRecognition(model, video_input):\n",
    "\n",
    "  vid = cv2.VideoCapture(video_input)\n",
    "  \n",
    "  transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "  if (vid.isOpened() == False):\n",
    "    print(\"An error occured while trying to read video\")\n",
    "  frame_width = int(vid.get(3))\n",
    "  frame_height = int(vid.get(4))\n",
    "\n",
    "\n",
    "  while(vid.isOpened()):\n",
    "    ret, frame = vid.read()\n",
    "    if ret == True:\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        \n",
    "        haar_cas = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces_detected = haar_cas.detectMultiScale(frame, scaleFactor = 1.2, minNeighbors=5)\n",
    "        \n",
    "        for (x, y, w, h) in faces_detected:\n",
    "          cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0),2)\n",
    "\n",
    "          converted_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "          trans_frame = transform(converted_image)\n",
    "          trans_frame.cuda()\n",
    "          model.cuda()\n",
    "\n",
    "        outputs = model(trans_frame.unsqueeze(0).cuda())\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "      emo = {0: \"Surprise\", 1: \"Fear\", 2: \"Disgust\", 3: \"Happiness\", 4: \"Sadness\", 5: \"Anger\", 6: \"Neutral\"}\n",
    "      cv2.putText(frame, emo[preds.item()], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0),2)\n",
    "      cv2_imshow(frame)\n",
    "      # out.write(frame)\n",
    "      if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    else:\n",
    "      break\n",
    "\n",
    "  vid.release()\n",
    "  cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfZPKOy3Pnjq"
   },
   "outputs": [],
   "source": [
    "# Download the VGG model class\n",
    "\n",
    "models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SInIzZNiPoQy"
   },
   "outputs": [],
   "source": [
    "# Loading in our CNN\n",
    "\n",
    "with open(\"/content/drive/MyDrive/CV_Submission/Test Functions/vgg_cnn.pkl\", \"rb\") as vgg:\n",
    "    vgg_model = pickle.load(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvlYQAisQ6ND"
   },
   "outputs": [],
   "source": [
    "# Here, we have MP4 videos that contain a video for each emotion\n",
    "\n",
    "happy_vid = \"/content/drive/MyDrive/CV_Submission/Videos/happy.mp4\"\n",
    "angry_vid = \"/content/drive/MyDrive/CV_Submission/Videos/angry.mp4\"\n",
    "neutral_vid = \"/content/drive/MyDrive/CV_Submission/Videos/neutral.mp4\"\n",
    "sad_vid = \"/content/drive/MyDrive/CV_Submission/Videos/sad.mp4\"\n",
    "surprised_vid = \"/content/drive/MyDrive/CV_Submission/Videos/suprised.mp4\"\n",
    "\n",
    "bond = \"/content/drive/MyDrive/CV_Submission/Videos/cr.mp4\"\n",
    "obama = \"/content/drive/MyDrive/CV_Submission/Videos/obama.mp4\"\n",
    "interstellar = \"/content/drive/MyDrive/CV_Submission/Videos/interstel.mp4\"\n",
    "trump = \"/content/drive/MyDrive/CV_Submission/Videos/trump2.mp4\"\n",
    "elon = \"/content/drive/MyDrive/CV_Submission/Videos/elon.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RyOI2KCT0CI"
   },
   "outputs": [],
   "source": [
    "# Testing our model\n",
    "\n",
    "EmotionRecognition(vgg_model, bond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsS2nfV4GtSK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMtCiOBaJdgTKee4q9MBhIu",
   "collapsed_sections": [],
   "name": "EmotionRecognition_Video.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
